@article{DBLP:journals/corr/abs-2301-07558,
  author    = {Yuting Ning and
               Zhenya Huang and
               Xin Lin and
               Enhong Chen and
               Shiwei Tong and
               Zheng Gong and
               Shijin Wang},
  title     = {Towards a Holistic Understanding of Mathematical Questions with Contrastive
               Pre-training},
  journal   = {AAAI},
  volume    = {abs/2301.07558},
  year      = {2023},
  url       = {https://doi.org/10.48550/arXiv.2301.07558},
  doi       = {10.48550/arXiv.2301.07558},
  eprinttype = {arXiv},
  eprint    = {2301.07558},
  timestamp = {Thu, 19 Jan 2023 15:40:01 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2301-07558.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{10.1145/3570725,
author = {He, Liyang and Huang, Zhenya and Chen, Enhong and Liu, Qi and Tong, Shiwei and Wang, Hao and Lian, Defu and Wang, Shijin},
title = {An Efficient and Robust Semantic Hashing Framework for Similar Text Search},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1046-8188},
url = {https://doi.org/10.1145/3570725},
doi = {10.1145/3570725},
abstract = {Similar text search aims to find texts relevant to a given query from a database, which is fundamental in many information retrieval applications, such as question search and exercise search. Since millions of texts always exist behind practical search engine systems, a well-developed text search system usually consists of recall and ranking stages. Specifically, the recall stage serves as the basis in the system, where the main purpose is to find a small set of relevant candidates accurately and efficiently. Towards this goal, deep semantic hashing, which projects original texts into compact hash codes, can support good search performance. However, learning desired textual hash codes is extremely difficult due to the following problems. First, compact hash codes (with short length) can improve retrieval efficiency, but the demand for learning compact hash codes cannot guarantee accuracy due to severe information loss. Second, existing methods always learn the unevenly distributed codes in the space from a local perspective, leading to unsatisfactory code-balance results. Third, a large fraction of textual data contains various types of noise in real-world applications, which causes the deviation of semantics in hash codes. To this end, in this paper, we first propose a general unsupervised encoder-decoder semantic hashing framework, namely MASH (short for Memory-bAsed Semantic Hashing), to learn the balanced and compact hash codes for similar text search. Specifically, with a target of retaining semantic information as much as possible, the encoder introduces a novel relevance constraint among informative high-dimensional representations to guide the compact hash code learning. Then, we design an external memory where the hashing learning can be optimized in the global space to ensure the code balance of the learning results, which can promote search efficiency. Besides, to alleviate the performance degradation problem of the model caused by text noise, we propose an improved SMASH (short for denoiSing Memory-bAsed Semantic Hashing) model by incorporating a noise-aware encoder-decoder framework. This framework considers the noise degree for each text from the semantic deviation aspect, ensuring the robustness of hash codes. Finally, we conduct extensive experiments in three real-world datasets. The experimental results clearly demonstrate the effectiveness and efficiency of MASH and SMASH in generating balanced and compact hash codes, as well as the superior denoising ability of SMASH.},
note = {Just Accepted},
journal = {ACM Trans. Inf. Syst.},
month = {jan},
keywords = {Semantic Hashing, Similarity Search, Efficient Codes, Robust Codes}
}
@ARTICLE{9894725,
  author={Huang, Wei and Chen, Enhong and Liu, Qi and Xiong, Hui and Huang, Zhenya and Tong, Shiwei and Zhang, Dan},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  title={HmcNet: A General Approach for Hierarchical Multi-Label Classification},
  year={2022},
  volume={},
  number={},
  pages={1-16},
  doi={10.1109/TKDE.2022.3207511}}

@inproceedings{DBLP:conf/sigir/TongWZTH022,
  author    = {Hanshuang Tong and
               Zhen Wang and
               Yun Zhou and
               Shiwei Tong and
               Wenyuan Han and
               Qi Liu},
  editor    = {Enrique Amig{\'{o}} and
               Pablo Castells and
               Julio Gonzalo and
               Ben Carterette and
               J. Shane Culpepper and
               Gabriella Kazai},
  title     = {Introducing Problem Schema with Hierarchical Exercise Graph for Knowledge
               Tracing},
  booktitle = {{SIGIR} '22: The 45th International {ACM} {SIGIR} Conference on Research
               and Development in Information Retrieval, Madrid, Spain, July 11 -
               15, 2022},
  pages     = {405--415},
  publisher = {{ACM}},
  year      = {2022},
  url       = {https://doi.org/10.1145/3477495.3532004},
  doi       = {10.1145/3477495.3532004},
  timestamp = {Sat, 09 Jul 2022 09:25:34 +0200},
  biburl    = {https://dblp.org/rec/conf/sigir/TongWZTH022.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@ARTICLE{9770424,
  author={Tao, Hanqing and Zhu, Guanqi and Chen, Enhong and Tong, Shiwei and Zhang, Kun and Xu, Tong and Liu, Qi and Ong, Yew-Soon},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  title={Learning from Ideography and Labels: A Schema-aware Radical-guided Associative Model for Chinese Text Classification},
  year={2022},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TKDE.2022.3171690}}

@inproceedings{10.1007/978-3-031-00129-1_14,
author = {Gong, Zheng and Tong, Shiwei and Wu, Han and Liu, Qi and Tao, Hanqing and Huang, Wei and Yu, Runlong},
title = {Tipster: A Topic-Guided Language Model for Topic-Aware Text Segmentation},
year = {2022},
isbn = {978-3-031-00128-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-00129-1_14},
doi = {10.1007/978-3-031-00129-1_14},
abstract = {The accurate segmentation and structural topics of plain documents not only meet people’s reading habit, but also facilitate various downstream tasks. Recently, some works have consistently given positive hints that text segmentation and segment topic labeling could be regarded as a mutual task, and cooperating with word distributions has the potential to model latent topics in a certain document better. To this end, we present a novel model namely Tipster to solve text segmentation and segment topic labeling collaboratively. We first utilize a neural topic model to infer latent topic distributions of sentences considering word distributions. Then, our model divides the document into topically coherent segments based on the topic-guided contextual sentence representations of the pre-trained language model and assign relevant topic labels to each segment. Finally, we conduct extensive experiments which demonstrate that Tipster achieves the state-of-the-art performance in both text segmentation and segment topic labeling tasks.},
booktitle = {Database Systems for Advanced Applications: 27th International Conference, DASFAA 2022, Virtual Event, April 11–14, 2022, Proceedings, Part III},
pages = {213–221},
numpages = {9},
keywords = {Text segmentation, Neural topic model, Language model}
}
@inproceedings{tao2021ideography,
  title={Ideography Leads Us to the Field of Cognition: A Radical-Guided Associative Model for Chinese Text Classification},
  author={Tao, Hanqing and Tong, Shiwei and Zhang, Kun and Xu, Tong and Liu, Qi and Chen, Enhong and Hou, Min},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={15},
  pages={13898--13906},
  year={2021},
  abstract={
    Cognitive psychology research shows that humans have the instinct for abstract thinking, where association plays an essential role in language comprehension. Especially for Chinese, its ideographic writing system allows radicals to trigger semantic association without the need of phonetics. In fact, subconsciously using the associative information guided by radicals is a key for readers to ensure the robustness of semantic understanding. Fortunately, many basic and extended concepts related to radicals are systematically included in Chinese language dictionaries, which leaves a handy but unexplored way for improving Chinese text representation and classification. To this end, we draw inspirations from cognitive principles between ideography and human associative behavior to propose a novel Radical-guided Associative Model (RAM) for Chinese text classification. RAM comprises two coupled spaces, namely Literal Space and Associative Space, which imitates the real process in people's mind when understanding a Chinese text. To be specific, we first devise a serialized modeling structure in Literal Space to thoroughly capture the sequential information of Chinese text. Then, based on the authoritative information provided by Chinese language dictionaries, we design an association module and put forward a strategy called Radical-Word Association to use ideographic radicals as the medium to associate prior concept words in Associative Space. Afterwards, we design an attention module to imitate people's matching and decision between Literal Space and Associative Space, which can balance the importance of each associative words under specific contexts. Finally, extensive experiments on two real-world datasets prove the effectiveness and rationality of RAM, with good cognitive insights for future language modeling.
  },
  abbr={AAAI'21},
  href={https://www.aaai.org/AAAI21Papers/AAAI-5126.TaoH.pdf},
}
@inproceedings{tao2019radical,
  title={A radical-aware attention-based model for chinese text classification},
  author={Tao, Hanqing and Tong, Shiwei and Zhao, Hongke and Xu, Tong and Jin, Binbin and Liu, Qi},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={5125--5132},
  year={2019},
  abbr={AAAI'19},
  abstract={
    Recent years, Chinese text
    classification has attracted more and more research
    attention. However, most existing techniques which
    specifically aim at English materials may lose effectiveness
    on this task due to the huge difference between
    Chinese and English. Actually, as a special kind of
    hieroglyphics, Chinese characters and radicals are
    semantically useful but still unexplored in the task of text
    classification. To that end, in this paper, we
    first analyze the motives of using multiple granularity
    features to represent a Chinese text
    by inspecting the characteristics of radicals, characters
    and words. For better representing the Chinese
    text and then implementing Chinese text classification, we
    propose a novel Radical-aware Attention-based
    Four-Granularity (RAFG) model to take full advantages of
    Chinese characters, words, character-level
    radicals, word-level radicals simultaneously. Specifically,
    RAFG applies a serialized BLSTM structure
    which is context-aware and able to capture the long-range
    information to model the character sharing
    property of Chinese and sequence characteristics in texts.
    Further, we design an attention mechanism to
    enhance the effects of radicals thus model the radical
    sharing property when integrating granularities.
    Finally, we conduct extensive experiments, where the
    experimental results not only show the superiority of
    our model, but also validate the effectiveness of radicals
    in the task of Chinese text classification
  },
  href={https://wvvw.aaai.org/ojs/index.php/AAAI/article/view/4446},
  address={Honolulu, Hawaii, USA}
}
@inproceedings{ma2021enhanced,
  title={Enhanced Representation Learning for Examination Papers with Hierarchical Document Structure},
  author={Ma, Yixiao and Tong, Shiwei and Liu, Ye and Wu, Likang and Liu, Qi and Chen, Enhong and Tong, Wei and Yan, Zi},
  booktitle={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2156--2160},
  year={2021},
  abstract={
  Representation learning of examination papers is the cornerstone of
    the Examination Paper Analysis (EPA) in education area including
    Paper Difficulty Prediction (PDR) and Finding Similar Papers (FSP).
    Previous works mainly focus on the representation learning of each
    test item, but few works notice the hierarchical document structure
    in examination papers. To this end, in this paper, we propose a novel
    Examination Organization Encoder (EOE) to learn a robust representation of the examination paper with the hierarchical document
    structure. Specifically, we first propose a syntax parser to recover
    the hierarchical document structure and convert an examination
    paper to an Examination Organization Tree (EOT), where the test
    items are the leaf nodes and the internal nodes are summarization
    of their child nodes. Then, we applied a two-layer GRU-based module to obtain the representation of each leaf node. After that, we
    design a subtree encoder module to aggregate the representation
    of each leaf node, which is used to calculate an embedding for each
    layer in the EOT. Finally, we feed all the layer embedding into an
    output module, the process is over and we get the examination
    paper representation that can be used for downstream tasks. Extensive experiments on real-world data demonstrate the effectiveness
    and interpretability of our method.
  }
}
@inproceedings{tong2020exploiting,
  title={Exploiting Knowledge Hierarchy for Finding Similar Exercises in Online Education Systems},
  author={Tong, Wei and Tong, Shiwei and Huang, Wei and He, Liyang and Ma, Jianhui and Liu, Qi and Chen, Enhong},
  abstract={
    In education systems, Finding Similar Exercises (FSE) is the key step for both exercise retrieval and duplicate detection. Recently, more and more attention has been drawn into this area and several works have been proposed, to utilize the exercise content (e.g., texts or images) or the labeled knowledge concepts. Such approaches, however, have failed to take knowledge hierarchy into account. To this end, we advance a novel knowledge-aware multimodal network, namely KnowNet, for finding similar exercises in large-scale online education systems by integrating the knowledge hierarchy into the heterogeneous exercise data and learning a relation-aware semantic representation. Specifically, we first propose a Content Representation Layer (CRL) to learn a unified semantic representation of the heterogeneous exercise content. Then, we design a Hierarchy Fusion Layer (HFL) to exploit the knowledge hierarchy. By combining the knowledge hierarchy, HFL can not only retrieve the relation-aware semantic representation but also provide an interpretable view to investigate the similarity of exercises. Finally, we adopt a Similarity Score Layer (SSL) for returning similar exercises. Extensive experiments demonstrate the effectiveness and interpretability of KnowNet.
  },
  booktitle={2020 IEEE International Conference on Data Mining (ICDM)},
  pages={1298--1303},
  year={2020},
  organization={IEEE}
}
@article{tao2019chinese,
  title={Chinese Embedding via Stroke and Glyph Information: A Dual-channel View},
  author={Tao, Hanqing and Tong, Shiwei and Xu, Tong and Liu, Qi and Chen, Enhong},
  journal={JOURNAL OF CHINESE INFORMATION PROCESSING},
  year={2019},
  abstract={
    Word embedding is a basic and very important topic in the field of Natural Language Processing. For Chinese,
    which has the nature of pictographic representation, it is urgent to explore more interpretable strategies
    to capture the language patterns in which morphological information is used to convey semantics. In this
    paper, we elaborate that Chinese word embeddings can be substantially enhanced by the morphological
    information hidden in
    characters which is reflected not only in strokes order sequentially, but also in character glyphs
    spatially. Then, we propose a novel Dual-channel Word Embedding model to realize the joint learning of
    sequential and spatial information of Chinese
    characters, so as to further enrich the representation of words. Through the evaluation on both word
    similarity and word analogy
    task, our model significantly outperforms other baseline methods and shows great interpretability.
  },
}
@inproceedings{huang2021stan,
  title={STAN: Adversarial Network for Cross-domain Question Difficulty Prediction},
  author={Huang, Ye and Huang, Wei and Tong, Shiwei and Huang, Zhenya and Liu, Qi and Chen, Enhong and Ma, Jianhui and Wan, Liang and Wang, Shijin},
  abstract={
    In intelligent education systems, question difficulty
    prediction (QDP) is a fundamental task of many applications,
    such as personalized question recommendation and test paper
    analysis. Previous work mainly focus on data-driven QDP methods, which are heavily relied on the large-scale labeled dataset of
    courses. To alleviate the labor intensity, an intuitive method is to
    introduce domain adaptation into QDP and consider each course
    as a domain. In educational psychology, there are two factors
    influencing difficulty common to different courses: the obstacles
    of comprehending the question and generating a response, namely
    stimulus and task difficulty. To this end, we propose a novel
    Stimulus and Task difficulty-based Adversarial Network (STAN)
    that models question difficulty from the views of stimulus and
    task. Then, in order to align the difficulty distribution of the
    source domain and the target domain, we utilize the conditional
    adversarial learning with readability-enhanced pseudo-labels.
    Meanwhile, we proposed a sampling method based on density
    estimation to implicit alignment. Finally, we conduct experiments
    on the real questions datasets to evaluate the effectiveness of
    our QDP model and domain adaptation method. Our method
    significantly improves accuracy over state-of-the-art methods on
    real-world question data of multiple courses.
  },
  booktitle={2021 IEEE International Conference on Data Mining (ICDM)},
  pages={220--229},
  year={2021},
  organization={IEEE}
}
@inproceedings{lei2021consistency,
  title={Consistency-aware Multi-modal Network for Hierarchical Multi-label Classification in Online Education System},
  author={Lei, Siqi and Huang, Wei and Tong, Shiwei and Liu, Qi and Huang, Zhenya and Chen, Enhong and Su, Yu},
  abstract={
    In the online education system, predicting the knowledge of exercises is a fundamental task of many applications, such as cognitive diagnosis. Usually, experts consider this problem as Hierarchical Multi-label Classification (HMC), since the knowledge concepts exhibit a multi-level structure. However, existing methods either sacrificed knowledge consistency for classification accuracy or sacrificed classification accuracy for knowledge consistency. Maintaining the balance is difficult. To forgo this dilemma, in this paper, we develop a novel frame-work called Consistency-Aware Multi-modal Network (Cam-Net). Specifically, we develop a multi-modal embedding module to learn the representation of the multi-modal exercise. Then, we adopt a hybrid prediction method consisting of the flat prediction module and the local prediction module. The local prediction module deals with the relation between the knowledge hierarchy and the input exercise. The flat prediction module focuses on maintaining knowledge consistency. Finally, to balance classification accuracy and knowledge consistency, we combine the outputs of two modules to make a final prediction. Extensive experimental results on two real-world datasets demonstrate the high performance and the ability to reduce knowledge inconsistency of CamNet.
  },
  booktitle={2021 IEEE International Conference on Big Knowledge (ICBK)},
  pages={1--8},
  year={2021},
  organization={IEEE},
  tag={Best Student Paper}
}
@article{fangguided,
  title={Guided Attention Network for Concept Extraction},
  author={Fang, Songtao and Huang, Zhenya and He, Ming and Tong, Shiwei and Huang, Xiaoqing and Liu, Ye and Huang, Jie and Liu, Qi},
  abstract={
    Concept extraction aims to find words or phrases
    describing a concept from massive texts. Recently,
    researchers propose many neural network-based
    methods to automatically extract concepts. Although these methods for this task show promising results, they ignore structured information in
    the raw textual data (e.g., title, topic, and clue
    words). In this paper, we propose a novel model,
    named Guided Attention Concept Extraction Network (GACEN), which uses title, topic, and clue
    words as additional supervision to provide guidance directly. Specifically, GACEN comprises two
    attention networks, one of them is to gather the
    relevant title and topic information for each context word in the document. The other one aims to
    model the implicit connection between informative
    words (clue words) and concepts. Finally, we aggregate information from two networks as input to
    Conditional Random Field (CRF) to model dependencies in the output. We collected clue words for
    three well-studied datasets. Extensive experiments
    demonstrate that our model outperforms the baseline models with a large margin.
  },
  booktitle={Inproceedings of 30th International Joint Conference on Artificial Intelligence},
  year={2021},
  abbr={IJCAI'21},
  href={https://www.ijcai.org/proceedings/2021/0200.pdf},
  address={Montreal-themed Virtual Reality}
}