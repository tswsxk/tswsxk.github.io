<!doctype html>
<html>
    {{ head }}
    <body>
        <div class="wrapper">
            {{sidebar}}

            <section>
                <h2>Published Papers</h2>
                <p><a style="margin:0; font-size:100%; font-weight:bold"
                      href="https://tswsxk.github.io/research/paper/RAFG_Tao.pdf">
                    A Radical-aware Attention-based Model for Chinese Text
                    Classification
                </a>
                    Hanqing Tao, <b>Shiwei Tong</b>, Hongke Zhao, Tong Xu,
                    Binbin Jin, Qi Liu,
                    <br>
                    In Proceedings of 33th AAAI Conference on Artificial
                    Intelligence (AAAI'19), Honolulu, Hawaii, USA, 2019,
                    Accepted.
                    <br>
                    <button class="accordion">
                        Abstract
                    </button>
                </p>
                <div class="panel"
                     style="background-color: #F1F1F1; color: #666; padding: 10px;">
                    Recent years, Chinese text
                    classification has attracted more and more research
                    attention. However, most existing techniques which
                    specifically aim at English materials may lose effectiveness
                    on this task due to the huge difference between
                    Chinese and English. Actually, as a special kind of
                    hieroglyphics, Chinese characters and radicals are
                    semantically useful but still unexplored in the task of text
                    classification. To that end, in this paper, we
                    first analyze the motives of using multiple granularity
                    features to represent a Chinese text
                    by inspecting the characteristics of radicals, characters
                    and words. For better representing the Chinese
                    text and then implementing Chinese text classification, we
                    propose a novel Radical-aware Attention-based
                    Four-Granularity (RAFG) model to take full advantages of
                    Chinese characters, words, character-level
                    radicals, word-level radicals simultaneously. Specifically,
                    RAFG applies a serialized BLSTM structure
                    which is context-aware and able to capture the long-range
                    information to model the character sharing
                    property of Chinese and sequence characteristics in texts.
                    Further, we design an attention mechanism to
                    enhance the effects of radicals thus model the radical
                    sharing property when integrating granularities.
                    Finally, we conduct extensive experiments, where the
                    experimental results not only show the superiority of
                    our model, but also validate the effectiveness of radicals
                    in the task of Chinese text classification
                </div>

                <hr>

                <h2>Notebook</h2>
                <ul>
                    <li>
                        <a href="https://tswsxk.github.io/research/survey/KG.pdf">
                            Knowledge Graph
                        </a>
                    </li>
                    <li>
                        <a href="https://tswsxk.github.io/research/survey/WE.pdf">
                            Language Representation
                        </a>
                    </li>
                </ul>

            </section>
            <script>
                var acc = document.getElementsByClassName("accordion");
                var i;

                for (i = 0; i < acc.length; i++) {
                    acc[i].onclick = function () {
                        this.classList.toggle("active");
                        this.parentNode.nextElementSibling.classList.toggle("show");
                    }
                }
            </script>
        </div>